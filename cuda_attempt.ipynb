{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b4a8ad4c627f4aad9da2cba0f40accad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_40d843903ace4b52873231bd579ed8d4","IPY_MODEL_56f7eff3723349768f2892bdc8fefdc0","IPY_MODEL_754f0938ecba471286e03998bcaa8301"],"layout":"IPY_MODEL_dbc566d3c9fa42d0b46c9228ae0727cf"}},"40d843903ace4b52873231bd579ed8d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31eba0d68e28474cbf339b3ecbb6361f","placeholder":"​","style":"IPY_MODEL_42281e6a6f8e4b52b48999a169bf3865","value":"config.json: 100%"}},"56f7eff3723349768f2892bdc8fefdc0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f41e5cb8188d4322b867a4f875537e75","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f0e0b901d9664bc582f22b754f9867df","value":665}},"754f0938ecba471286e03998bcaa8301":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81c1f175d292466f8cc33af88f67e64b","placeholder":"​","style":"IPY_MODEL_8831aa33d12b4358b2d1a6cba37a9b68","value":" 665/665 [00:00&lt;00:00, 21.8kB/s]"}},"dbc566d3c9fa42d0b46c9228ae0727cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31eba0d68e28474cbf339b3ecbb6361f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42281e6a6f8e4b52b48999a169bf3865":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f41e5cb8188d4322b867a4f875537e75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0e0b901d9664bc582f22b754f9867df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"81c1f175d292466f8cc33af88f67e64b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8831aa33d12b4358b2d1a6cba37a9b68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe628553dd864973bf20cf3ce65079b4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_97ad8596ca4e451fab1e27b77042bff2","IPY_MODEL_d546eab1455c4d13a76c6ac7866e97d8","IPY_MODEL_dc12da618aac480e8af10e6c1d952b23"],"layout":"IPY_MODEL_eb696ba947a648bebf3e49cd46035e1a"}},"97ad8596ca4e451fab1e27b77042bff2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e34eae510bd45c6851c807133050fbc","placeholder":"​","style":"IPY_MODEL_cd19335c162e43e3be36837774788809","value":"model.safetensors: 100%"}},"d546eab1455c4d13a76c6ac7866e97d8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9959f4ae610b4e9e8ecc8e46a8c5a695","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1bdcab6db320413bacad142669c3dae9","value":548105171}},"dc12da618aac480e8af10e6c1d952b23":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e680250581c49a1a834d85a90fada2d","placeholder":"​","style":"IPY_MODEL_b7b53a4c6d2848d4b1475e95e22f2b6b","value":" 548M/548M [00:07&lt;00:00, 54.5MB/s]"}},"eb696ba947a648bebf3e49cd46035e1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e34eae510bd45c6851c807133050fbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd19335c162e43e3be36837774788809":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9959f4ae610b4e9e8ecc8e46a8c5a695":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bdcab6db320413bacad142669c3dae9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e680250581c49a1a834d85a90fada2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7b53a4c6d2848d4b1475e95e22f2b6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e2d449b69bb48cc9a3bdd4dd2d15ced":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_efc4df58d52842e0af3137a382ab6e1d","IPY_MODEL_f0b700a699fd467a80f181751655bc03","IPY_MODEL_f80d1deae6f7410b9310632f28b9028e"],"layout":"IPY_MODEL_59fe6e2f0bbb4f608de68910d5202849"}},"efc4df58d52842e0af3137a382ab6e1d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d845a17383954464b27462173cc8fa06","placeholder":"​","style":"IPY_MODEL_7d0c7a330ccc4422b7cb83d6b6c361e1","value":"generation_config.json: 100%"}},"f0b700a699fd467a80f181751655bc03":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e19008db44d4fbca9128140f7b1a028","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_abd7d7f1ff3e4972ba7bf8cbf0294fd9","value":124}},"f80d1deae6f7410b9310632f28b9028e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34a79bed6abd48f794b2c698fb5aabd7","placeholder":"​","style":"IPY_MODEL_b718964f1414467ca3f770546f7b412a","value":" 124/124 [00:00&lt;00:00, 8.30kB/s]"}},"59fe6e2f0bbb4f608de68910d5202849":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d845a17383954464b27462173cc8fa06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d0c7a330ccc4422b7cb83d6b6c361e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e19008db44d4fbca9128140f7b1a028":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abd7d7f1ff3e4972ba7bf8cbf0294fd9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"34a79bed6abd48f794b2c698fb5aabd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b718964f1414467ca3f770546f7b412a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pycuda","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tn85uNsAv417","outputId":"4e32a26c-57b3-4ff2-a672-969ae9ec2733","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport numpy as np\n#try:\n#  import pycuda.driver as cuda\n#  import pycuda.autoinit\n#  from pycuda.compiler import SourceModule\n#except ImportError:\n#   pass\n#try:\n#  import Metal\n#except ImportError:\n#  print(\"couldnt get the metal\")\n#  pass\n\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nfrom pycuda.compiler import SourceModule\n\nclass buffer:\n  def __init__(self,data,size,d):\n      self.data = data\n      self.size = size\n      self.d = d\n      self.np_data = None #cache?\n      #TODO cache np if faster?\n\n  def np(self,params=None):\n    if self.d == \"Metal\": #TODO np_cache\n        output = np.asarray(self.data.contents().as_buffer(self.size))\n        return np.frombuffer(output, dtype=np.float32)\n    if self.d == \"OpenCL\":\n        queue = params[\"queue\"]\n        if self.np_data is None:\n            self.np_data = np.zeros(math.ceil(self.size/4)).astype(np.float32)\n        cl.enqueue_copy(queue, self.np_data, self.data)\n        return self.np_data\n    if self.d == \"CUDA\":\n        if self.np_data is None:\n            self.np_data = np.zeros(math.ceil(self.size/4)).astype(np.float32)\n        cuda.memcpy_dtoh(self.np_data, self.data)\n        return self.np_data\n\n  def copy(self,params):\n    return create_buffer(self.np(params),self.d,params)\n\n  def rand_like(x,params):\n        return create_buffer(np.random.random(np.shape(x.np(params).flatten())).astype(np.float32),x.d,params)\n\n  def delete(self): #todo OpenCL\n      if self.d == \"Metal\":\n        self.data.setPurgeableState_(Metal.MTLPurgeableStateEmpty)\n        self.data.release()\n\n\ndef compile(prg_str,d,params):\n  if d == \"Metal\":\n    library, err = params[\"device\"].newLibraryWithSource_options_error_(prg_str, Metal.MTLCompileOptions.alloc().init(), None)\n    return library\n  if d == \"OpenCL\":\n    return cl.Program(params[\"ctx\"],prg_str).build()\n  if d == \"CUDA\":\n    return SourceModule(prg_str)\n\ndef run(prg,func,params,args,gs,ls,d):\n  if d == \"Metal\":\n    mtl_queue = params[\"queue\"]\n    device = params[\"device\"]\n    fxn = prg.newFunctionWithName_(func)\n    command_buffer = mtl_queue.commandBuffer()\n    encoder = command_buffer.computeCommandEncoder()\n    pipeline_state, err = device.newComputePipelineStateWithFunction_error_(fxn, None)\n    encoder.setComputePipelineState_(pipeline_state)\n    i = 0\n    for arg in args:\n        encoder.setBuffer_offset_atIndex_(arg.data, 0, i)\n        i+=1\n    threadsPerGrid = Metal.MTLSizeMake(gs,1,1)\n    threadsPerThreadGroup = Metal.MTLSizeMake(ls,1,1)\n    encoder.dispatchThreadgroups_threadsPerThreadgroup_(threadsPerGrid, threadsPerThreadGroup)\n    encoder.endEncoding()\n    command_buffer.commit()\n    command_buffer.waitUntilCompleted()\n  if d == \"OpenCL\":\n     gs*=ls\n     queue = params[\"queue\"]\n     kernel = getattr(prg,func)\n     data = []\n     for a in args: data.append(a.data) #todo, better way?\n     kernel(queue, (gs,1), (ls,1),*data)\n  if d == \"CUDA\":\n    fxn = prg.get_function(func)\n    data = []\n    for a in args: data.append(a.data) #todo, better way?\n    fxn(*data,block=(ls,1,1),grid=(gs,1))\n  return\n\ndef run_test(prg,func,params,args,gs,ls,d): #TODO, only for metal because no delete in OpenCL yet\n  args_copy_a = []\n  for a in args:\n    args_copy_a.append(a.copy(params))\n  run(prg,func,params,args_copy_a,gs,ls,d)\n  for x in range(3):\n    print(\"test =\",x)\n    args_copy_b = []\n    for a in args:\n        args_copy_b.append(a.copy(params))\n    run(prg,func,params,args_copy_b,gs,ls,d)\n    for j in range(len(args_copy_b)):\n      np.testing.assert_allclose(args_copy_a[j].np(params),args_copy_b[j].np(params),1e-6)\n    for j in range(len(args_copy_b)):\n      args_copy_b[j].delete()\n    args_copy_b = [] #todo, needed?\n  for j in range(len(args_copy_a)):\n    args_copy_a[j].delete()\n  args_copy_a = []#todo, needed?\n  run(prg,func,params,args,gs,ls,d)\n  return\n\ndef run_old(prg,func,params,args,gs,ls,d):\n  if d == \"Metal\":\n    mtl_queue = params[\"queue\"]\n    device = params[\"device\"]\n    fxn = prg.newFunctionWithName_(func)\n    command_buffer = mtl_queue.commandBuffer()\n    encoder = command_buffer.computeCommandEncoder()\n    pipeline_state, err = device.newComputePipelineStateWithFunction_error_(fxn, None)\n    encoder.setComputePipelineState_(pipeline_state)\n    i = 0\n    for arg in args:\n        encoder.setBuffer_offset_atIndex_(arg.data, 0, i)\n        i+=1\n    threadsPerGrid = Metal.MTLSizeMake(gs,1,1)\n    threadsPerThreadGroup = Metal.MTLSizeMake(ls,1,1)\n    encoder.dispatchThreadgroups_threadsPerThreadgroup_(threadsPerGrid, threadsPerThreadGroup)\n    encoder.endEncoding()\n    command_buffer.commit()\n    command_buffer.waitUntilCompleted()\n  if d == \"OpenCL\":\n     queue = params[\"queue\"]\n     kernel = getattr(prg,func)\n     kernel(queue, (gs,1), (ls,1),*args)\n  return\n\ndef create_buffer(a,d,params):\n  if d == \"Metal\":\n    device = params[\"device\"]\n    a_buffer = device.newBufferWithLength_options_(len(a.flatten())*4 ,1)\n    m = a_buffer.contents().as_buffer(len(a.flatten())*4)\n    m[:] = bytes(a)\n    return buffer(a_buffer,len(a.flatten())*4,d)\n  if d == \"OpenCL\":\n    ctx = params[\"ctx\"]\n    mf = params[\"mf\"]\n    data = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=a)\n    return buffer(data,len(a.flatten()),d)\n  if d == \"CUDA\":\n    a_gpu = cuda.mem_alloc(a.nbytes)\n    cuda.memcpy_htod(a_gpu, a)\n    return buffer(a_gpu,len(a.flatten()),d)\n  return None\n\ndef create_buffer_empty(size,d,params):\n    if d == \"Metal\":\n        device = params[\"device\"]\n        a_buffer = device.newBufferWithLength_options_(size ,1)\n        return buffer(a_buffer,size,d)\n    if d == \"OpenCL\":\n        ctx = params[\"ctx\"]\n        mf = params[\"mf\"]\n        data = cl.Buffer(ctx, mf.READ_ONLY, size)\n        return buffer(data,size,d)\n    if d == \"CUDA\":\n        return buffer(cuda.mem_alloc(size),size,d)\n    return None\n","metadata":{"id":"_chRjL5es43W","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport time\nimport math\nls = 256\n\nkernel_prefix = {\"OpenCL\":\"\",\"Metal\":\"#include <metal_stdlib>\\n#include <metal_simdgroup_matrix>\\nusing namespace metal;\\n\",\"CUDA\":\"\"}\nuint3_arg = {\"OpenCL\":\"\",\"Metal\":\", uint3 gid [[thread_position_in_grid]]\",\"CUDA\":\"\"}\nfunc_dec = {\"OpenCL\":\"__kernel\",\"Metal\":\"kernel\",\"CUDA\":\"__global__\"} #TODO vs local cuda?\nvar_dec = {\"OpenCL\":\"__global\",\"Metal\":\"device\",\"CUDA\":\"\"}\nbarrier = {\"OpenCL\":\"barrier(CLK_LOCAL_MEM_FENCE);\",\"Metal\":\"threadgroup_barrier(mem_flags::mem_threadgroup);\",\"CUDA\":\" __syncthreads();\"}\nglobal_idx = {\"OpenCL\":\"get_global_id(0)\",\"Metal\":\"gid.x\",\"CUDA\":\"threadIdx.x+blockIdx.x*blockDim.x\"}\nlocal_var = {\"OpenCL\":\"__attribute__ ((aligned (16))) __local\",\"Metal\":\"threadgroup\",\"CUDA\":\"__shared__\"}\n\n\n\nclass Kernels:\n    def __init__(self,dim,n_heads,max_context,device):\n        self.d = device\n        self.prg_cache = {}\n        self.dim = dim\n        self.n_heads = n_heads\n        self.max_context = max_context\n        if device == \"Metal\":\n            self.device = Metal.MTLCreateSystemDefaultDevice()\n            self.queue = self.device.newCommandQueue()\n            self.params = {\"queue\":self.queue,\"device\":self.device}\n        if device == \"OpenCL\":\n            platform = cl.get_platforms()\n            my_gpu_devices = platform[0].get_devices(device_type=cl.device_type.GPU)\n            ctx = cl.Context(devices=my_gpu_devices)\n            self.queue = cl.CommandQueue(ctx)\n            mf = cl.mem_flags\n            prg = None\n            self.params = {\"ctx\":ctx,\"mf\":mf,\"queue\":self.queue}\n        if device == \"CUDA\":\n            self.params = None\n\n    def add(self,a_g,b_g,b_s=0,a_s=0):\n        if hasattr(self, 'add_res_g') == False:\n            self.add_res_g = create_buffer_empty(self.dim*4,self.d,self.params)\n        prg_str = f\"\"\"\n        {kernel_prefix[self.d]}\n        {func_dec[self.d]} void add(\n            {var_dec[self.d]} const float *a, {var_dec[self.d]} const float *b, {var_dec[self.d]} float *res{uint3_arg[self.d]})\n        {{\n        int gidx0 = {global_idx[self.d]};\n        if(gidx0 < {self.dim}) {{\n            res[gidx0] = a[{int(a_s)*self.dim} + gidx0] + b[gidx0 + {b_s*self.dim}];\n        }}\n        }}\n        \"\"\"\n        prg = compile(prg_str,self.d,self.params)\n        g = math.ceil(self.dim / ls)\n        run(prg,\"add\",self.params,[a_g, b_g,self.add_res_g],g,ls,self.d) #TODO this breaks it all (CUDA?)\n        return self.add_res_g\n\n    def tok_emb(self,tokens,weight_g,weight_2_g,no_tokens):\n        tokens_g = create_buffer(tokens.astype(np.int32),self.d,self.params)\n        size = no_tokens*self.dim\n        tok_emb_g = create_buffer_empty(no_tokens*self.dim*4,self.d,self.params)\n        prg_str = f\"\"\"\n        {kernel_prefix[self.d]}\n        {func_dec[self.d]} void mm(\n            {var_dec[self.d]} int *tokens, {var_dec[self.d]} const float *weight, {var_dec[self.d]} const float *weight2,  {var_dec[self.d]} float *tok_emb{uint3_arg[self.d]})\n        {{\n            int gidx0 = {global_idx[self.d]};\n            int i = gidx0 / {self.dim};\n            int j = gidx0 % {self.dim};\n            tok_emb[i*{self.dim} + j] = weight[tokens[i]*{self.dim} + j] + weight2[i*{self.dim} + j];\n        }}\n        \"\"\"\n        library = compile(prg_str,self.d,self.params)\n        gs = math.ceil(size / ls)\n        run(library,\"mm\",self.params,[tokens_g,weight_g,weight_2_g,tok_emb_g],gs,ls,self.d)\n        return tok_emb_g\n\n    def kernel_1(self,h_g,weight_g,bias_g,weight2_g,temperature,random_num):\n        ls = 256\n        seg = int(self.dim / ls)\n        rows = self.dim\n        cols = 50257\n        if hasattr(self, 'logits_g') == False:\n            self.logits_g = create_buffer_empty(50257*4,self.d,self.params)\n        if hasattr(self, 'res') == False:\n            self.res = np.zeros(1).astype(np.float32)\n        if hasattr(self, 'res_g') == False:\n            self.res_g = create_buffer_empty(1*4,self.d,self.params)\n        seg2 = math.ceil(50257 / ls)\n        prg_str = f\"\"\"\n        {kernel_prefix[self.d]}\n        {func_dec[self.d]} void mm4(\n            {var_dec[self.d]} float *h, {var_dec[self.d]} const float *weight, {var_dec[self.d]} const float *bias{uint3_arg[self.d]})\n        {{\n            {local_var[self.d]} float temp[{ls}];\n            {local_var[self.d]} float mean;\n            int lidx0 = {global_idx[self.d]};\n            float total = 0;\n            for(int i = 0; i < {seg}; i++) {{\n                total += h[lidx0*{seg} + i];\n            }}\n            temp[lidx0] = total;\n            {barrier[self.d]}\n            if(lidx0==0) {{\n                total = 0;\n                for(int i = 0; i < {ls}; i++) {{\n                    total += temp[i];\n                }}\n                mean = total / {self.dim};\n            }}\n            {barrier[self.d]}\n            for(int i = 0; i < {seg}; i++) {{\n                h[i + lidx0*{seg}] -= mean;\n            }}\n            {barrier[self.d]}\n            total = 0;\n            for(int i = 0; i < {seg}; i++) {{\n                total += pow(h[lidx0*{seg} + i],2);\n            }}\n            temp[lidx0] = total;\n            {barrier[self.d]}\n            if(lidx0==0) {{\n                total = 0;\n                for(int i = 0; i < {ls}; i++) {{\n                    total += temp[i];\n                }}\n                mean = pow(total / {self.dim} + 1e-5,0.5);\n            }}\n            {barrier[self.d]}\n            for(int i = 0; i < {seg}; i++) {{\n                h[i + lidx0*{seg}] = (h[i + lidx0*{seg}] * weight[i + lidx0*{seg}]) / mean + bias[i + lidx0*{seg}];\n            }}\n        }}\n        {func_dec[self.d]} void matvec(\n            {var_dec[self.d]} const float *h, {var_dec[self.d]} const float *weight2 , {var_dec[self.d]} float *res{uint3_arg[self.d]})\n        {{\n            int gidx0 = {global_idx[self.d]};\n            res[gidx0] = 0;\n            for(int j = 0; j < {rows}; j++) {{\n                res[gidx0] += h[j] * weight2[gidx0 + j*{cols}];\n            }}\n            res[gidx0] /= {temperature};\n        }}\n        {func_dec[self.d]} void mm5(\n            {var_dec[self.d]} const float *a, {var_dec[self.d]} float *res{uint3_arg[self.d]})\n        {{\n            res[0] = a[0]; //todo why is this needed?, used to be a MAX\n        }}\n\n        {func_dec[self.d]} void mm6(\n        {var_dec[self.d]} float *a, {var_dec[self.d]} float *res{uint3_arg[self.d]})\n        {{\n            int gidx0 = {global_idx[self.d]};\n            a[gidx0] = exp(a[gidx0] - res[0]);\n        }}\n\n        {func_dec[self.d]} void mm8(\n        {var_dec[self.d]} float *a, {var_dec[self.d]} const float *res{uint3_arg[self.d]})\n        {{\n            int gidx0 = {global_idx[self.d]};\n            a[gidx0] = a[gidx0] / res[0];\n        }}\n\n        {func_dec[self.d]} void mm9(\n        {var_dec[self.d]} const float *a, {var_dec[self.d]} float *res{uint3_arg[self.d]})\n        {{\n            {local_var[self.d]} float temp[{ls}];\n            int lidx0 = {global_idx[self.d]};\n            temp[lidx0] = 0;\n            for(int i = 0; i < {math.ceil(50257 / ls)}; i++) {{\n                if(lidx0*{math.ceil(50257 / ls)} + i < 50257){{\n                temp[lidx0] += a[lidx0*{math.ceil(50257 / ls)} + i];\n                }}\n            }}\n            {barrier[self.d]}\n            float t = 0;\n            if(lidx0 == 0) {{\n                for(int i = 0; i < {ls}; i++) {{\n                    t+=temp[i];\n                }}\n                res[0] = t;\n            }}\n        }}\n\n        {func_dec[self.d]} void mm10(\n        {var_dec[self.d]} float *a{uint3_arg[self.d]})\n        {{\n            for(int i = 1; i < 50257; i++) {{\n                a[i] += a[i-1];\n            }}\n        }}\n        \"\"\"\n\n        if prg_str not in self.prg_cache:\n            library = compile(prg_str,self.d,self.params)\n            self.prg_cache[prg_str] = library\n        prg = self.prg_cache[prg_str]\n\n        prg_str = f\"\"\"\n        {func_dec[self.d]} void mm11(\n        {var_dec[self.d]} float *a{uint3_arg[self.d]})\n        {{\n            int gidx0 = {global_idx[self.d]};\n            if(a[gidx0] < {random_num}) {{ //TODO, used to be (a[gidx0] / a[50256])/{random_num}\n                a[gidx0] = 1;\n            }} else {{\n                a[gidx0] = 0;\n            }}\n        }}\n        \"\"\"\n        prg2 = compile(prg_str,self.d,self.params)\n\n        gs =  math.ceil(50257 / ls)\n        run(prg,\"mm4\",self.params,[h_g, weight_g, bias_g],1,ls,self.d)\n        run(prg,\"matvec\",self.params,[h_g, weight2_g,self.logits_g],gs,ls,self.d)\n        run(prg,\"mm5\",self.params,[self.logits_g,self.res_g],1,1,self.d)\n        run(prg,\"mm6\",self.params,[self.logits_g,self.res_g],gs,ls,self.d)\n        run(prg,\"mm5\",self.params,[self.logits_g,self.res_g],1,1,self.d)\n        run(prg,\"mm8\",self.params,[self.logits_g,self.res_g],gs,ls,self.d)\n        run(prg,\"mm9\",self.params,[self.logits_g,self.res_g],1,ls,self.d)\n        run(prg,\"mm8\",self.params,[self.logits_g,self.res_g],gs,ls,self.d)\n        run(prg,\"mm10\",self.params,[self.logits_g],1,1,self.d)\n        run(prg2,\"mm11\",self.params,[self.logits_g],gs,ls,self.d)\n        run(prg,\"mm9\",self.params,[self.logits_g,self.res_g],1,ls,self.d)\n        return self.res_g.np(self.params)\n\n    def kernel_3(self,x_g,weight_g,bias_g,attn_weight_g,attn_bias_g,new_cache_g\\\n        ,ln_f_weight_g,ln_f_bias_g,n_tokens,max_content,lm_head_weight_g,temperature,random_num):\n        ls = 256\n        size = self.dim\n        b_cols2 = 50257\n        b_rows2 = self.dim\n        seg2 = math.ceil(50257 / ls)\n        b_cols = self.dim*3 #todo\n        b_rows = self.dim\n        seg = int(size / ls) #todo\n        x0_g = create_buffer_empty(n_tokens*self.dim*4,self.d,self.params)\n        logits_g = create_buffer_empty(50257*4,self.d,self.params)\n        c_g = create_buffer_empty(max_content*b_cols*4,self.d,self.params) #todo, can this be smaller?\n        res = np.zeros(1).astype(np.float32)\n        res_g = create_buffer_empty(1*4,self.d,self.params)\n\n        prg_str = f\"\"\"\n        {kernel_prefix[self.d]}\n        {func_dec[self.d]} void mm({var_dec[self.d]} const float *x_in,\n            {var_dec[self.d]} float *x, {var_dec[self.d]} const float *weight, {var_dec[self.d]} const float *bias{uint3_arg[self.d]})\n        {{\n            {local_var[self.d]} float temp[{ls}];\n            {local_var[self.d]} float temp2[{n_tokens}];\n            int gidx0 = {global_idx[self.d]};\n            int lidx0 = gidx0 % {ls};\n            int r = gidx0 / {ls};\n            temp2[r] = 0;\n            for(int i = 0; i < {seg}; i++) {{\n                x[{self.dim}*r + lidx0*{seg} + i] = x_in[{self.dim}*r + lidx0*{seg} + i];\n                temp2[r] += x[{self.dim}*r + lidx0*{seg} + i];\n            }}\n            temp[lidx0] = temp2[r];\n            {barrier[self.d]}\n            if(lidx0<{n_tokens}) {{\n                temp2[lidx0] = 0;\n                for(int i = 0; i < {ls}; i++) {{\n                    temp2[lidx0] += temp[i];\n                }}\n                temp2[lidx0] = temp2[lidx0] / {size};\n            }}\n            {barrier[self.d]}\n            for(int i = 0; i < {seg}; i++) {{\n                x[{self.dim}*r + i + lidx0*{seg}] -= temp2[r];\n            }}\n            {barrier[self.d]}\n            temp2[r] = 0;\n            for(int i = 0; i < {seg}; i++) {{\n                temp2[r] += pow(x[{self.dim}*r + lidx0*{seg} + i],2);\n            }}\n            temp[lidx0] = temp2[r];\n            {barrier[self.d]}\n            if(lidx0<{n_tokens}) {{\n                temp2[lidx0] = 0;\n                for(int i = 0; i < {ls}; i++) {{\n                    temp2[lidx0] += temp[i];\n                }}\n                temp2[lidx0] = pow(temp2[lidx0] / {size} + 1e-5,0.5);\n            }}\n            {barrier[self.d]}\n            for(int i = 0; i < {seg}; i++) {{\n                x[{self.dim}*r + i + lidx0*{seg}] = (x[{self.dim}*r + i + lidx0*{seg}] * weight[i + lidx0*{seg}]) / temp2[r] + bias[i + lidx0*{seg}];\n            }}\n        }}\n        {func_dec[self.d]} void mm2(\n            {var_dec[self.d]} const float *x, {var_dec[self.d]} const float *attn_weight, {var_dec[self.d]} const float *attn_bias,{var_dec[self.d]} float *res{uint3_arg[self.d]})\n        {{\n            if({global_idx[self.d]} < {b_cols*n_tokens}) {{ //TODO\n            int gidx0 = {global_idx[self.d]};\n            int i = gidx0 / {n_tokens};\n            int y = gidx0 % {n_tokens};\n            float total = 0;\n            for(int k = 0; k < {b_rows}; k++) {{\n                total += x[y*{b_rows} + k] * attn_weight[i*{b_rows} + k];\n            }}\n            res[y*{b_cols} + i] = total + attn_bias[i];\n            }}\n        }}\n        {func_dec[self.d]} void mm3(\n            {var_dec[self.d]} const float *xqkv, {var_dec[self.d]} float *new_cache{uint3_arg[self.d]})\n        {{\n            int gidx0 = {global_idx[self.d]};\n            int i = gidx0 / {self.n_heads*64};\n            int j = gidx0 % {self.n_heads*64};\n            new_cache[i*{self.n_heads*64} + j] = xqkv[i*{self.n_heads*64*3} + {self.dim*1} + j];\n            new_cache[{max_content*self.n_heads*64} + i*{self.n_heads*64} + j] = xqkv[i*{self.n_heads*64*3} + {self.dim}*2 + j];\n        }}\n         {func_dec[self.d]} void mm4(\n            {var_dec[self.d]} const float *xqkv, {var_dec[self.d]} float *new_cache{uint3_arg[self.d]})\n        {{\n            if({global_idx[self.d]} < {b_cols*n_tokens}) {{\n            int gidx0 = {global_idx[self.d]};\n            int i = gidx0 / {self.n_heads*64};\n            int j = gidx0 % {self.n_heads*64};\n            new_cache[i*{self.n_heads*64} + j] = xqkv[i*{self.n_heads*64*3} + j + {self.dim}];\n            new_cache[{max_content*self.n_heads*64} + i*{self.n_heads*64} + j] = xqkv[i*{self.n_heads*64*3} + j + {self.dim}*2];\n            }}\n        }}\n        {func_dec[self.d]} void mm5(\n            {var_dec[self.d]} float *x, {var_dec[self.d]} const float *ln_f_weight, {var_dec[self.d]} const float *ln_f_bias{uint3_arg[self.d]})\n        {{\n            {local_var[self.d]} float temp[{ls}];\n            {local_var[self.d]} float mean;\n            int lidx0 = {global_idx[self.d]};\n            float total = 0;\n            for(int i = 0; i < {seg}; i++) {{\n                total += x[lidx0*{seg} + i];\n            }}\n            temp[lidx0] = total;\n            {barrier[self.d]}\n            if(lidx0==0) {{\n                total = 0;\n                for(int i = 0; i < {ls}; i++) {{\n                    total += temp[i];\n                }}\n                mean = total / {size};\n            }}\n            {barrier[self.d]}\n            for(int i = 0; i < {seg}; i++) {{\n                x[i + lidx0*{seg} + {(n_tokens - 1)*self.dim}] -= mean;\n            }}\n            {barrier[self.d]}\n            total = 0;\n            for(int i = 0; i < {seg}; i++) {{\n                total += pow(x[lidx0*{seg} + i + {(n_tokens - 1)*self.dim}],2);\n            }}\n            temp[lidx0] = total;\n            {barrier[self.d]}\n            if(lidx0==0) {{\n                total = 0;\n                for(int i = 0; i < {ls}; i++) {{\n                    total += temp[i];\n                }}\n                mean = pow(total / {size} + 1e-5,0.5);\n            }}\n            {barrier[self.d]}\n            for(int i = 0; i < {seg}; i++) {{\n                x[i + lidx0*{seg}] = (x[i + lidx0*{seg} + {(n_tokens - 1)*self.dim}] * ln_f_weight[i + lidx0*{seg}]) / mean + ln_f_bias[i + lidx0*{seg}];\n            }}\n        }}\n        {func_dec[self.d]} void matmul(\n            {var_dec[self.d]} const float *a, {var_dec[self.d]} const float *b, {var_dec[self.d]} float *res{uint3_arg[self.d]})\n        {{\n            if({global_idx[self.d]} < {b_cols2}) {{\n                int x = {global_idx[self.d]};\n                float total = 0;\n                for(int k = 0; k < {b_rows2}; k++) {{\n                    total += a[k] * b[x*{b_rows2} + k];\n                }}\n                res[x] = total / {temperature};\n            }}\n        }}\n        {func_dec[self.d]} void mm6(\n            {var_dec[self.d]} const float *a, {var_dec[self.d]} float *res{uint3_arg[self.d]})\n        {{\n            res[0] = a[0]; //todo why is this needed?, used to be a MAX\n        }}\n\n        {func_dec[self.d]} void mm7(\n        {var_dec[self.d]} float *a, {var_dec[self.d]} const float *res{uint3_arg[self.d]})\n        {{\n            if({global_idx[self.d]} < 50257) {{ //TODO\n            int gidx0 = {global_idx[self.d]};\n            a[gidx0] = exp(a[gidx0] - res[0]);\n            }}\n        }}\n\n        {func_dec[self.d]} void mm9(\n        {var_dec[self.d]} float *a, {var_dec[self.d]} const float *res{uint3_arg[self.d]})\n        {{\n            if({global_idx[self.d]} < 50257) {{\n            int gidx0 = {global_idx[self.d]};\n            a[gidx0] = a[gidx0] / res[0];\n            }}\n        }}\n\n        {func_dec[self.d]} void mm10(\n        {var_dec[self.d]} const float *a, {var_dec[self.d]} float *res{uint3_arg[self.d]})\n        {{\n            {local_var[self.d]} float temp[{ls}];\n            int lidx0 = {global_idx[self.d]};\n            float t = 0;\n            for(int i = 0; i < {seg2}; i++) {{\n                if(lidx0*{seg2} + i < 50257) {{\n                t += a[lidx0*{seg2} + i];\n                }}\n            }}\n            temp[lidx0] = t;\n            {barrier[self.d]}\n            if(lidx0 == 0) {{\n                t = 0;\n                for(int i = 0; i < {ls}; i++) {{\n                    t += temp[i];\n                }}\n                res[0] = t;\n            }}\n        }}\n\n        {func_dec[self.d]} void mm11(\n        {var_dec[self.d]} float *a{uint3_arg[self.d]})\n        {{\n            for(int i = 1; i < 50257; i++) {{\n                a[i] += a[i-1];\n            }}\n        }}\n\n        {func_dec[self.d]} void mm12(\n        {var_dec[self.d]} float *a{uint3_arg[self.d]})\n        {{\n            int gidx0 = {global_idx[self.d]};\n            if(a[gidx0] < {random_num}) {{ //TODO, used to be (a[gidx0] / a[50256])/{random_num}\n                a[gidx0] = 1;\n            }} else {{\n                a[gidx0] = 0;\n            }}\n        }}\n        \"\"\"\n\n        if prg_str not in self.prg_cache:\n            library = compile(prg_str,self.d,self.params)\n            self.prg_cache[prg_str] = library\n        prg = self.prg_cache[prg_str]\n\n        run(prg,\"mm\",self.params,[x_g, x0_g, weight_g, bias_g],n_tokens,ls,self.d)\n        gs = math.ceil(b_cols*n_tokens / ls)\n        run(prg,\"mm2\",self.params,[x0_g, attn_weight_g,attn_bias_g,c_g],gs,ls,self.d)\n        run(prg,\"mm4\",self.params,[c_g, new_cache_g],gs,ls,self.d)\n        run(prg,\"mm5\",self.params,[x_g, ln_f_weight_g, ln_f_bias_g],1,ls,self.d)\n        run(prg,\"matmul\",self.params,[x_g, lm_head_weight_g,logits_g],math.ceil(b_cols2 / ls),ls,self.d)\n        run(prg,\"mm6\",self.params,[logits_g,res_g],1,1,self.d)\n        gs = math.ceil(50257 / ls)\n        run(prg,\"mm7\",self.params,[logits_g,res_g],gs,ls,self.d)\n        run(prg,\"mm6\",self.params,[logits_g,res_g],1,1,self.d)\n        run(prg,\"mm9\",self.params,[logits_g,res_g],gs,ls,self.d)\n        run(prg,\"mm10\",self.params,[logits_g,res_g],1,ls,self.d)\n        run(prg,\"mm9\",self.params,[logits_g,res_g],gs,ls,self.d)\n        run(prg,\"mm11\",self.params,[logits_g],1,1,self.d)\n        run(prg,\"mm12\",self.params,[logits_g],gs,ls,self.d)\n        run(prg,\"mm10\",self.params,[logits_g,res_g],1,ls,self.d)\n        return res_g.np(self.params)\n\n    def kernel_0(self,a_g,c_g,d_g,e_g,xqkv_g,keys_values_g,weight_g,bias_g,\\\n        weight2_g,bias2_g,weight3_g,bias3_g,weight4_g,bias4_g,start_pos,g,j=0):\n        ls = 256\n        ls = 256 #TODO why is 256 fastet than 32?\n        seg3 = math.ceil(self.dim / ls) #todo\n        seg = math.ceil(self.dim / ls)\n        if hasattr(self, 'temp_g') == False:\n            self.temp_g = create_buffer_empty(self.n_heads*self.max_context*4,self.d,self.params)\n        if hasattr(self, 'xq_temp_g') == False:\n            self.xq_temp_g = create_buffer_empty((self.n_heads*(self.max_context+1)*64 + 64)*4,self.d,self.params) #TODO can this be smaller?\n        #{barrier[self.d]}\n        prg_str = f\"\"\"\n        {kernel_prefix[self.d]}\n        {func_dec[self.d]} void mm(\n            {var_dec[self.d]} float *a,\n            {var_dec[self.d]} float *mean{uint3_arg[self.d]})\n        {{\n            {local_var[self.d]} float temp[{ls}];\n            int lidx0 = {global_idx[self.d]};\n            float t = 0;\n            for(int i = 0; i < {seg}; i++) {{\n                t += a[lidx0*{seg} + i];\n            }}\n            temp[lidx0] = t;\n            {barrier[self.d]}\n            if(lidx0==0) {{\n                t = 0;\n                for(int i = 0; i < {ls}; i++) {{\n                    t += temp[i];\n                }}\n                mean[0] = t / {self.dim};\n            }}\n            {barrier[self.d]}\n            t = 0;\n            for(int i = 0; i < {seg}; i++) {{\n                a[i + lidx0*{seg}] -= mean[0];\n                t += pow(a[lidx0*{seg} + i],2);\n            }}\n            temp[lidx0] = t;\n            {barrier[self.d]}\n            if(lidx0==0) {{\n                t = 0;\n                for(int i = 0; i < {ls}; i++) {{\n                    t += temp[i];\n                }}\n                mean[0] = pow(t / {self.dim} + 1e-5,0.5);\n            }}\n        }}\n\n        {func_dec[self.d]} void mm4(\n            {var_dec[self.d]} float *a,\n            {var_dec[self.d]} const float *weight2, {var_dec[self.d]} const float *bias2,\n            {var_dec[self.d]} const float *weight3, {var_dec[self.d]} const float *bias3,\n            {var_dec[self.d]} float *mean,\n            {var_dec[self.d]} float *h_temp, {var_dec[self.d]} float *h, {var_dec[self.d]} float *bias3_temp{uint3_arg[self.d]})\n        {{\n            int lidx0 = {global_idx[self.d]} % {ls};\n            int i = {global_idx[self.d]} / {ls};\n            bias3_temp[i + lidx0*{math.ceil(self.dim*4 / ls)}] = bias3[i + lidx0*{math.ceil(self.dim*4 / ls)}];\n            for(int j = 0; j < {self.dim}; j++) {{\n                bias3_temp[i + lidx0*{math.ceil(self.dim*4 / ls)}] += ((h[j] * weight2[j]) / mean[0] + bias2[j]) * weight3[(i + lidx0*{math.ceil(self.dim*4 / ls)})*{self.dim} + j];\n            }}\n            float tth = bias3_temp[i + lidx0*{math.ceil(self.dim*4 / ls)}] * 0.7978845608\\\n            * (1 + 0.044715 * pow(bias3_temp[i + lidx0*{math.ceil(self.dim*4 / ls)}],2));\n            float th = tanh(tth);\n            if(isnan(th) && tth < 0) {{ th = -1;}}\n            if(isnan(th) && tth >= 0) {{ th = 1;}}\n            bias3_temp[i + lidx0*{math.ceil(self.dim*4 / ls)}] = 0.5 * bias3_temp[i + lidx0*{math.ceil(self.dim*4 / ls)}]\\\n            * (1 + th);\n        }}\n        {func_dec[self.d]} void mm5(\n            {var_dec[self.d]} float *a,\n            {var_dec[self.d]} const float *weight4,{var_dec[self.d]} const float *bias4,\n            {var_dec[self.d]} float *h_temp, {var_dec[self.d]} float *bias3_temp{uint3_arg[self.d]})\n        {{\n            {local_var[self.d]} float bias4_temp[{self.dim*3}];\n            int lidx0 = {global_idx[self.d]} % {ls};\n            int i = {global_idx[self.d]} / {ls};\n            bias4_temp[lidx0 + i*{ls}] = bias4[lidx0 + i*{ls}];\n            for(int j = 0; j < {self.dim*4}; j++) {{\n                bias4_temp[lidx0 + i*{ls}] += bias3_temp[j] * weight4[lidx0 + i*{ls} + j*{self.dim}];\n            }}\n            a[lidx0 + i*{ls}] = bias4_temp[lidx0 + i*{ls}] + h_temp[lidx0 + i*{ls}];\n        }}\n        \"\"\"\n\n        if prg_str not in self.prg_cache:\n            library = compile(prg_str,self.d,self.params)\n            self.prg_cache[prg_str] = library\n        prg = self.prg_cache[prg_str]\n\n        prg_str = f\"\"\"\n        {kernel_prefix[self.d]}\n        {func_dec[self.d]} void mm1(\n            {var_dec[self.d]} const float *a, {var_dec[self.d]} const float *c, {var_dec[self.d]} const float *d, {var_dec[self.d]} const float *e,\n            {var_dec[self.d]} const float *xqkv, {var_dec[self.d]} float *keys_values,\n            {var_dec[self.d]} float *xq_temp, {var_dec[self.d]} float *mean{uint3_arg[self.d]})\n        {{\n            int lidx0 = {global_idx[self.d]} % {ls};\n            int i = {global_idx[self.d]} / {ls};\n            float t = 0;\n            xq_temp[lidx0*{math.ceil(self.dim*3 / ls)} + i] = xqkv[lidx0*{int(self.dim*3 / ls)} + i];\n            for(int k = 0; k < {self.dim}; k++) {{\n                t += ((a[k] * c[k]) / mean[0] + d[k]) * e[(lidx0*{int(self.dim*3 / ls)} + i)*{self.dim} + k];\n            }}\n            if((lidx0*{int(self.dim*3 / ls)} + i) < {g}) {{\n                xq_temp[lidx0*{int(self.dim*3 / ls)} + i] += t;\n                }}\n            if((lidx0*{int(self.dim*3 / ls)} + i) >= {g} && (lidx0*{int(self.dim*3 / ls)} + i) < {2*g}) {{\n                keys_values[{start_pos}*{self.dim} + lidx0*{int(self.dim*3 / ls)} + i - {g}] = xqkv[{self.dim} + lidx0*{int(self.dim*3 / ls)} + i - {g}] + t;\n            }}\n            if((lidx0*{int(self.dim*3 / ls)} + i) >= {2*g}) {{\n                keys_values[{self.dim*self.max_context} + {start_pos}*{self.dim} + lidx0*{int(self.dim*3 / ls)} + i - {2*g}] = xqkv[{self.dim*2} + lidx0*{int(self.dim*3 / ls)} + i - {2*g}] + t;\n            }}\n        }}\n        {func_dec[self.d]} void mm2(\n            {var_dec[self.d]} const float *keys_values, {var_dec[self.d]} float *temp3, {var_dec[self.d]} const float *xq_temp{uint3_arg[self.d]})\n        {{\n            if({global_idx[self.d]} < {self.n_heads*(start_pos+1)*(start_pos+1)}) {{\n            int lidx0 = {global_idx[self.d]};\n            int x = (lidx0) % {start_pos+1};\n            int k = (lidx0) / {start_pos+1};\n            float acc0 = 0;\n            for(int i = 0; i < 64; i++) {{\n                acc0 += xq_temp[i + 64*k] * keys_values[x*{self.n_heads*64} + i + 64*k];\n            }}\n            if(x + k*{start_pos+1} < {self.n_heads*self.max_context}) {{\n            temp3[x + k*{start_pos+1}] = acc0 / 8; //hardcoded math.sqrt(64)\n            }}\n            }}\n        }}\n        {func_dec[self.d]} void mm3(\n            {var_dec[self.d]} float *a,\n            {var_dec[self.d]} float *keys_values,\n            {var_dec[self.d]} const float *weight,{var_dec[self.d]} const float *bias,\n            {var_dec[self.d]} const float *weight2, {var_dec[self.d]} const float *bias2,\n            {var_dec[self.d]} const float *weight3, {var_dec[self.d]} const float *bias3,\n            {var_dec[self.d]} const float *weight4,\n            {var_dec[self.d]} float *bias4,\n            {var_dec[self.d]} float *temp3, {var_dec[self.d]} float *xq_temp, {var_dec[self.d]} float *mean,\n            {var_dec[self.d]} float *h_temp, {var_dec[self.d]} float *h{uint3_arg[self.d]})\n        {{\n            {local_var[self.d]} float temp[{ls}];\n            int lidx0 = {global_idx[self.d]};\n            if(lidx0 < {self.n_heads}){{\n            float m = -INFINITY;\n            for(int i = 0; i < {start_pos+1}; i++) {{\n                m = max(m,temp3[i + lidx0*{start_pos+1}]);\n            }}\n            float t = 0;\n            for(int i = 0; i < {start_pos+1}; i++) {{\n                temp3[i + lidx0*{start_pos+1}] = exp(temp3[i + lidx0*{start_pos+1}] - m);\n                t += temp3[i + lidx0*{start_pos+1}];\n            }}\n            for(int i = 0; i < {start_pos+1}; i++) {{\n                temp3[i + lidx0*{start_pos+1}] /= t;\n            }}\n            }}\n            {barrier[self.d]}\n            for(int g = 0; g < {seg3}; g++) {{\n                float acc0 = 0;\n                for(int i = 0; i < {start_pos+1}; i++) {{\n                    acc0 += temp3[i + {start_pos+1}*((g + lidx0*{seg3}) / 64)] * keys_values[{self.dim*self.max_context} + i*{self.n_heads*64} + g + lidx0*{seg3}];\n                }}\n                xq_temp[g + lidx0*{seg3}] = acc0;\n            }}\n            {barrier[self.d]}\n            for(int i = 0; i < {seg3}; i++) {{\n                float acc = 0;\n                for(int x = 0; x < {self.dim}; x++) {{\n                    acc += xq_temp[x] * weight[x*{self.dim} + lidx0*{seg3} + i];\n                }}\n                h[lidx0*{seg3} + i] = a[lidx0*{seg3} + i] + acc + bias[lidx0*{seg3} + i];\n                h_temp[lidx0*{seg3} + i] = h[lidx0*{seg3} + i];\n            }}\n            {barrier[self.d]}\n            float total = 0;\n            for(int i = 0; i < {seg3}; i++) {{\n                total += h[lidx0*{seg3} + i];\n            }}\n            temp[lidx0] = total;\n            {barrier[self.d]}\n            if(lidx0==0) {{\n                total = 0;\n                for(int i = 0; i < {ls}; i++) {{\n                    total += temp[i];\n                }}\n                mean[0] = total / {self.dim};\n            }}\n            {barrier[self.d]}\n            total = 0;\n            for(int i = 0; i < {seg3}; i++) {{\n                h[i + lidx0*{seg3}] = h[i + lidx0*{seg3}] - mean[0];\n                total += pow(h[lidx0*{seg3} + i],2);\n            }}\n            temp[lidx0] = total;\n            {barrier[self.d]}\n            if(lidx0==0) {{\n                total = 0;\n                for(int i = 0; i < {ls}; i++) {{\n                    total += temp[i];\n                }}\n                mean[0] = pow(total / {self.dim} + 1e-5,0.5);\n            }}\n            }}\n        \"\"\"\n        if prg_str not in self.prg_cache:\n            library = compile(prg_str,self.d,self.params)\n            self.prg_cache[prg_str] = library\n        prg2 = self.prg_cache[prg_str]\n\n        if hasattr(self, 'total') == False:\n            self.total = create_buffer_empty(1*4,self.d,self.params)\n\n        if hasattr(self, 'bias3_temp') == False:\n            self.bias3_temp = create_buffer_empty(self.dim*4*4,self.d,self.params)\n        if hasattr(self, 'mean') == False:\n            self.mean = create_buffer_empty(1*4,self.d,self.params)\n        if hasattr(self, 'h_temp') == False:\n            self.h_temp = create_buffer_empty(self.dim*4,self.d,self.params)\n        if hasattr(self, 'h') == False:\n            self.h = create_buffer_empty(self.dim*4,self.d,self.params)\n\n        run(prg,\"mm\",self.params,[a_g,self.mean],1,ls,self.d)\n        run(prg2,\"mm1\",self.params,[a_g,c_g,d_g,e_g,xqkv_g,keys_values_g,self.xq_temp_g,self.mean],math.ceil(self.dim*3 / ls),ls,self.d)\n        run(prg2,\"mm2\",self.params,[keys_values_g ,self.temp_g, self.xq_temp_g],math.ceil((self.n_heads*(start_pos+1)*(start_pos+1)) / ls),ls,self.d)\n        run(prg2,\"mm3\",self.params,[a_g,keys_values_g,weight_g\\\n        ,bias_g,weight2_g,bias2_g,weight3_g,bias3_g,weight4_g,bias4_g,self.temp_g, self.xq_temp_g,self.mean,self.h_temp,self.h],1,ls,self.d)\n        run(prg,\"mm4\",self.params,[a_g,weight2_g,bias2_g,\\\n        weight3_g,bias3_g,self.mean,self.h_temp,self.h,self.bias3_temp],int(self.dim*4 / ls),ls,self.d)\n        run(prg,\"mm5\",self.params,[a_g,weight4_g,bias4_g,self.h_temp,self.bias3_temp],int(self.dim / ls),ls,self.d)\n        return a_g\n\n    def kernel_2(self,x_g,ln_1_weight_g,ln_1_bias_g,attn_weight_g,attn_bias_g,cache_kv_g,attn_c_proj_weight_g,attn_c_proj_bias_g,ln_2_weight_g,ln_2_bias_g,c_fc_weight_g,c_fc_bias_g\\\n        ,c_proj_weight_g,c_proj_bias_g,num_tokens,max_content,j=0):\n        if hasattr(self, 'h_g') == False:\n            self.h_g = create_buffer_empty(max_content*self.dim*4,self.d,self.params)\n        if hasattr(self, 'h2_g') == False:\n            self.h2_g = create_buffer_empty(max_content*self.dim*4,self.d,self.params)\n        if hasattr(self, 'xq_g') == False:\n            self.xq_g = create_buffer_empty(self.n_heads*64*max_content*4,self.d,self.params)\n        if hasattr(self, 'xq_g_temp') == False:\n            self.xq_g_temp = create_buffer_empty(self.n_heads*64*max_content*4,self.d,self.params)\n        if hasattr(self, 'xv_g') == False:\n            self.xv_g = create_buffer_empty(self.n_heads*64*max_content*4,self.d,self.params)\n        if hasattr(self, 'c_g') == False:\n            self.c_g = create_buffer_empty(self.n_heads*64*max_content*4,self.d,self.params)\n        if hasattr(self, 'xqt_g') == False:\n            self.xqt_g = create_buffer_empty(self.n_heads*64*max_content*4,self.d,self.params)\n        if hasattr(self, 'res_g') == False:\n            self.res_g = create_buffer_empty(max_content*self.n_heads*4,self.d,self.params)\n        if hasattr(self, 'xqkv_g') == False:\n            self.xqkv_g = create_buffer_empty(max_content*self.dim*3*4,self.d,self.params)\n        if hasattr(self, 'd_g') == False:\n            self.d_g = create_buffer_empty(max_content*self.dim*4*4,self.d,self.params)\n        a_rows = num_tokens\n        a_cols = 64\n        b_rows = self.dim\n        ls = 256\n        size = self.dim\n        seg = int(size / ls) #todo\n        b_cols = self.dim*3 # for first part\n        b_cols_2 = self.dim*4\n        prg_str = f\"\"\"\n        {kernel_prefix[self.d]}\n        {func_dec[self.d]} void mm(\n            {var_dec[self.d]} float *x, {var_dec[self.d]} const float *weight, {var_dec[self.d]} const float *bias,\n            {var_dec[self.d]} float *copy{uint3_arg[self.d]})\n        {{\n            {local_var[self.d]} float temp[{ls}];\n            {local_var[self.d]} float temp2[{num_tokens}];\n            int gidx0 = {global_idx[self.d]};\n            int lidx0 = gidx0 % {ls};\n            int r = gidx0 / {ls};\n            temp2[r] = 0;\n            for(int i = 0; i < {seg}; i++) {{\n                copy[{self.dim}*r + lidx0*{seg} + i] = x[{self.dim}*r + lidx0*{seg} + i];\n                temp2[r] += x[{self.dim}*r + lidx0*{seg} + i];\n            }}\n            temp[lidx0] = temp2[r];\n            {barrier[self.d]}\n            if(lidx0<{num_tokens}) {{\n                temp2[lidx0] = 0;\n                for(int i = 0; i < {ls}; i++) {{\n                    temp2[lidx0] += temp[i];\n                }}\n                temp2[lidx0] = temp2[lidx0] / {size};\n            }}\n            {barrier[self.d]}\n            for(int i = 0; i < {seg}; i++) {{\n                x[{self.dim}*r + i + lidx0*{seg}] -= temp2[r];\n            }}\n            {barrier[self.d]}\n            temp2[r] = 0;\n            for(int i = 0; i < {seg}; i++) {{\n                temp2[r] += pow(x[{self.dim}*r + lidx0*{seg} + i],2.0);\n            }}\n            temp[lidx0] = temp2[r];\n            {barrier[self.d]}\n            if(lidx0<{num_tokens}) {{\n                temp2[lidx0] = 0;\n                for(int i = 0; i < {ls}; i++) {{\n                    temp2[lidx0] += temp[i];\n                }}\n                temp2[lidx0] = pow(temp2[lidx0] / {size} + 1e-5,0.5);\n            }}\n            {barrier[self.d]}\n            for(int i = 0; i < {seg}; i++) {{\n                x[{self.dim}*r + i + lidx0*{seg}] = (x[{self.dim}*r + i + lidx0*{seg}] * weight[i + lidx0*{seg}]) / temp2[r] + bias[i + lidx0*{seg}];\n            }}\n        }}\n        {func_dec[self.d]} void mm2(\n            {var_dec[self.d]} const float *x, {var_dec[self.d]} const float *attn_weight, {var_dec[self.d]} const float *attn_bias,{var_dec[self.d]} float *res{uint3_arg[self.d]})\n        {{\n            int gidx0 = {global_idx[self.d]};\n            if(gidx0 < {b_cols*num_tokens}) {{\n            int i = gidx0 / {num_tokens};\n            int y = gidx0 % {num_tokens};\n            float total = 0;\n            for(int k = 0; k < {b_rows}; k++) {{\n                total += x[y*{b_rows} + k] * attn_weight[i*{b_rows} + k];\n            }}\n            res[y*{b_cols} + i] = total + attn_bias[i];\n            }}\n        }}\n        {func_dec[self.d]} void mm3(\n            {var_dec[self.d]} const float *xqkv, {var_dec[self.d]} float *new_cache{uint3_arg[self.d]})\n        {{\n            int gidx0 = {global_idx[self.d]};\n            int i = gidx0 / {self.n_heads*64};\n            int j = gidx0 % {self.n_heads*64};\n            new_cache[i*{self.n_heads*64} + j] = xqkv[i*{self.n_heads*64*3} + {self.dim}*1 + j];\n            new_cache[{max_content}*{self.n_heads*64} + i*{self.n_heads*64} + j] = xqkv[i*{self.n_heads*64*3} + {self.dim}*2 + j];\n        }}\n        {func_dec[self.d]} void tr(\n            {var_dec[self.d]} const float *xqkv, {var_dec[self.d]} float *xq, {var_dec[self.d]} float *xv{uint3_arg[self.d]})\n        {{\n            int gidx0 = {global_idx[self.d]};\n            int i = (gidx0 / {64}) / {num_tokens};\n            int j = (gidx0 / {64}) % {num_tokens};\n            int k = gidx0 % 64;\n            xq[i*{num_tokens}*64 + j*64 + k] = xqkv[i*64 + j*{64*self.n_heads*3} + k];\n            xv[i*{num_tokens}*64 + j*64 + k] = xqkv[i*64 + j*{64*self.n_heads*3} + k + {64*self.n_heads*2}];\n        }}\n        {func_dec[self.d]} void ms0(\n            {var_dec[self.d]} float *xq_temp, {var_dec[self.d]} const float *xq, {var_dec[self.d]} const float *xqkv{uint3_arg[self.d]})\n        {{\n                int gidx0 = {global_idx[self.d]};\n                if(gidx0 < {self.n_heads*a_rows*a_rows}) {{\n                int x = (gidx0 / {a_rows}) % {a_rows};\n                int z = gidx0 / ({a_rows}*{a_rows});\n                int y = gidx0 % {a_rows};\n                float total = 0;\n                for(int k = 0; k < {a_cols}; k++) {{\n                    total += xq[y*{a_cols} + k + z*{a_rows}*{a_cols}] * xqkv[x*{64*self.n_heads*3} + k + z*64 + {self.dim}];\n                }}\n                xq_temp[y*{a_rows} + x + z*{a_rows}*{a_rows}] = total / 8; //sqrt 64 input shape xq\n                }}\n        }}\n        {func_dec[self.d]} void ms(\n            {var_dec[self.d]} float *xq{uint3_arg[self.d]})\n        {{\n        int gidx0 = {global_idx[self.d]};\n        if(gidx0 < {self.n_heads*num_tokens*num_tokens}) {{\n            int x = (gidx0 / {num_tokens}) / {num_tokens};\n            int y = (gidx0 / {num_tokens}) % {num_tokens};\n            int z = gidx0 % {num_tokens};\n            if(z > y) {{ //todo, this can probably be 2x faster\n                xq[x*{num_tokens}*{num_tokens} + y*{num_tokens} + z] = -INFINITY;\n            }}\n            xq[x*{num_tokens}*{num_tokens} + y*{num_tokens} + z] = exp(xq[x*{num_tokens}*{num_tokens} + y*{num_tokens} + z]);\n        }}\n        }}\n        {func_dec[self.d]} void ms3(\n            {var_dec[self.d]} const float *xq, {var_dec[self.d]} float *mx{uint3_arg[self.d]})\n        {{\n        int gidx0 = {global_idx[self.d]};\n        if(gidx0 < {num_tokens*self.n_heads}) {{\n        int x = gidx0 / {num_tokens};\n        int y = gidx0 % {num_tokens};\n            float m = 0;\n            for(int z = 0; z < {num_tokens}; z++) {{\n                m += xq[x*{num_tokens}*{num_tokens} + y*{num_tokens} + z];\n            }}\n            mx[x*{num_tokens} + y] = m;\n            }}\n        }}\n        {func_dec[self.d]} void ms4(\n            {var_dec[self.d]} float *xq, {var_dec[self.d]} const float *mx{uint3_arg[self.d]})\n        {{\n        int gidx0 = {global_idx[self.d]};\n        if(gidx0 < {num_tokens*num_tokens*self.n_heads}) {{\n            int x = (gidx0 / {num_tokens}) / {num_tokens};\n            int y = (gidx0 / {num_tokens}) % {num_tokens};\n            int z = gidx0 % {num_tokens};\n            xq[x*{num_tokens}*{num_tokens} + y*{num_tokens} + z] /= mx[x*{num_tokens} + y];\n        }}\n        }}\n        {func_dec[self.d]} void ms5(\n            {var_dec[self.d]} const float *xq, {var_dec[self.d]} const float *xv, {var_dec[self.d]} float *res{uint3_arg[self.d]})\n        {{\n            int gidx0 = {global_idx[self.d]};\n            int z = (gidx0 / {num_tokens}) / {a_cols};\n            int x = (gidx0 / {num_tokens}) % {a_cols};\n            int y = gidx0 % {num_tokens};\n            float total = 0;\n            for(int k = 0; k < {num_tokens}; k++) {{\n                total += xq[y*{num_tokens} + k + z*{num_tokens}*{num_tokens}] * xv[x + k*{a_cols} + z*{num_tokens}*{a_cols}];\n            }}\n            res[y*{a_cols} + x + z*{a_cols}*{num_tokens}] = total;\n        }}\n        {func_dec[self.d]} void ms6( //transpose\n            {var_dec[self.d]} const float *xq, {var_dec[self.d]} float *xqt{uint3_arg[self.d]})\n        {{\n            int gidx0 = {global_idx[self.d]};\n            int i = (gidx0 / 64) / {num_tokens};\n            int j = (gidx0 / 64) % {num_tokens};\n            int k = gidx0 % 64;\n            xqt[i*64 + j*{self.n_heads*64} + k] = xq[i*{num_tokens}*64 + j*64 + k];\n        }}\n        {func_dec[self.d]} void ms7(\n            {var_dec[self.d]} const float *xq, {var_dec[self.d]} const float *attn_weight,{var_dec[self.d]} const float *attn_bias, {var_dec[self.d]} float *res{uint3_arg[self.d]})\n        {{\n            if({global_idx[self.d]} < {b_rows*num_tokens}) {{ //TODO don't allow larger local size? wasteful\n            int gidx0 = {global_idx[self.d]};\n            int x = gidx0 / {num_tokens};\n            int y = gidx0 % {num_tokens};\n            float total = 0;\n            for(int k = 0; k < {b_rows}; k++) {{\n                total += xq[y*{b_rows} + k] * attn_weight[x*{b_rows} + k];\n            }}\n            res[y*{b_rows} + x] += total + attn_bias[x];\n            }}\n        }}\n\n        {func_dec[self.d]} void ms8( //TODO, there are other kernels like this to fix\n            {var_dec[self.d]} float *x, {var_dec[self.d]} const float *ln_2_weight, {var_dec[self.d]} const float *ln_2_bias\n            ,{var_dec[self.d]} float *copy{uint3_arg[self.d]})\n        {{\n            {local_var[self.d]} float temp[{ls}];\n            {local_var[self.d]} float total;\n            int gidx0 = {global_idx[self.d]};\n            int lidx0 = gidx0 % {ls};\n            int r = gidx0 / {ls}; //todo clean\n            total = 0;\n            for(int i = 0; i < {seg}; i++) {{\n                copy[{self.dim}*r + lidx0*{seg} + i] = x[{self.dim}*r + lidx0*{seg} + i];\n                total += x[{self.dim}*r + lidx0*{seg} + i];\n            }}\n            temp[lidx0] = total;\n            {barrier[self.d]}\n            if(lidx0<{num_tokens}) {{\n                total = 0;\n                for(int i = 0; i < {ls}; i++) {{\n                    total += temp[i];\n                }}\n            }}\n            {barrier[self.d]}\n            for(int i = 0; i < {seg}; i++) {{\n                x[{self.dim}*r + i + lidx0*{seg}] -= total / {size};\n            }}\n            {barrier[self.d]}\n            total = 0;\n            for(int i = 0; i < {seg}; i++) {{\n                total += pow(x[{self.dim}*r + lidx0*{seg} + i],2);\n            }}\n            temp[lidx0] = total;\n            {barrier[self.d]}\n            if(lidx0<{num_tokens}) {{\n                total = 0;\n                for(int i = 0; i < {ls}; i++) {{\n                    total += temp[i];\n                }}\n                total = pow(total / {size} + 1e-5,0.5);\n            }}\n            {barrier[self.d]}\n            for(int i = 0; i < {seg}; i++) {{\n                x[{self.dim}*r + i + lidx0*{seg}] = (x[{self.dim}*r + i + lidx0*{seg}] * ln_2_weight[i + lidx0*{seg}]) / total + ln_2_bias[i + lidx0*{seg}];\n            }}\n        }}\n\n        {func_dec[self.d]} void ms9(\n            {var_dec[self.d]} const float *a, {var_dec[self.d]} const float *c_fc_weight,{var_dec[self.d]} const float *c_fc_bias, {var_dec[self.d]} float *res{uint3_arg[self.d]})\n        {{\n            int gidx0 = {global_idx[self.d]};\n            int x = gidx0 / {num_tokens};\n            int y = gidx0 % {num_tokens};\n            float total = 0;\n            for(int k = 0; k < {b_rows}; k++) {{\n                total += a[y*{b_rows} + k] * c_fc_weight[x*{b_rows} + k];  //TODO A LEADS TO NANs\n            }}\n            float tth = (total + c_fc_bias[x]) * 0.7978845608\\\n                * (1 + 0.044715 * pow((total + c_fc_bias[x]),2));\n            float th = tanh(tth);\n            if(isnan(th) && tth < 0) {{\n                th = -1;\n            }}\n            if(isnan(th) && tth >= 0) {{\n                th = 1;\n            }}\n            res[y*{b_cols_2} + x] = 0.5 * (total + c_fc_bias[x])\\\n                * (1 + th);\n        }}\n        {func_dec[self.d]} void ms10(\n            {var_dec[self.d]} const float *a, {var_dec[self.d]} const float *c_proj_weight,{var_dec[self.d]} const float *c_proj_bias, {var_dec[self.d]} float *res{uint3_arg[self.d]})\n        {{\n            if({global_idx[self.d]} < {b_rows*num_tokens}) {{ //TODO, wasteful?\n            int gidx0 = {global_idx[self.d]};\n            int x = gidx0 / {num_tokens};\n            int y = gidx0 % {num_tokens};\n            float total = 0;\n            for(int k = 0; k < {b_cols_2}; k++) {{\n                total += a[y*{b_cols_2} + k] * c_proj_weight[x*{b_cols_2} + k];\n            }}\n            res[y*{b_rows} + x] += total + c_proj_bias[x];\n            }}\n        }}\n        \"\"\"\n        #if prg_str not in self.prg_cache: TODO, why does caching this change the output?\n        #    library = compile(prg_str,self.d,self.params)\n        #    self.prg_cache[prg_str] = library\n        #prg = self.prg_cache[prg_str]\n\n        prg = compile(prg_str,self.d,self.params)\n        run(prg,\"mm\",self.params,[x_g,ln_1_weight_g,ln_1_bias_g,self.h_g],num_tokens,ls,self.d)\n        print(\"output =\",x_g.np()[0:10])\n        run(prg,\"mm2\",self.params,[x_g,attn_weight_g,attn_bias_g,self.xqkv_g],math.ceil(b_cols*num_tokens / ls),ls,self.d)\n        run(prg,\"mm3\",self.params,[self.xqkv_g, cache_kv_g],math.ceil((num_tokens*self.n_heads*64) / ls),ls,self.d)\n        run(prg,\"tr\",self.params,[self.xqkv_g, self.xq_g, self.xv_g],math.ceil((num_tokens*self.n_heads*64) / ls),ls,self.d)\n        run(prg,\"ms0\",self.params,[self.xq_g_temp,self.xq_g, self.xqkv_g],math.ceil(self.n_heads*num_tokens*num_tokens/ls),ls,self.d)\n        run(prg,\"ms\",self.params,[self.xq_g_temp],math.ceil(self.n_heads*num_tokens*num_tokens / ls),ls,self.d)\n        run(prg,\"ms3\",self.params,[self.xq_g_temp,self.res_g],math.ceil(self.n_heads*num_tokens/ls),min(self.n_heads*num_tokens,ls),self.d)\n        run(prg,\"ms4\",self.params,[self.xq_g_temp,self.res_g],math.ceil(self.n_heads*num_tokens*num_tokens / ls),ls,self.d)\n        run(prg,\"ms5\",self.params,[self.xq_g_temp,self.xv_g,self.c_g],math.ceil(self.n_heads*a_cols*num_tokens / ls),ls,self.d)\n        run(prg,\"ms6\",self.params,[self.c_g,self.xqt_g],math.ceil(num_tokens*self.n_heads*64 / ls),ls,self.d)\n        run(prg,\"ms7\",self.params,[self.xqt_g,attn_c_proj_weight_g,attn_c_proj_bias_g,self.h_g],math.ceil(b_rows*num_tokens / ls),ls,self.d)\n        run(prg,\"ms8\",self.params,[self.h_g, ln_2_weight_g, ln_2_bias_g,self.h2_g],num_tokens,ls,self.d)\n        run(prg,\"ms9\",self.params,[self.h_g, c_fc_weight_g,c_fc_bias_g,self.d_g],math.ceil(b_cols_2*num_tokens / ls),ls,self.d)\n        run(prg,\"ms10\",self.params,[self.d_g, c_proj_weight_g,c_proj_bias_g,self.h2_g],math.ceil(b_rows*num_tokens / ls) ,ls,self.d)\n        return self.h2_g\n\n    def time_it(func,a,b,i=100):\n        f = None\n        total_time = 0\n        for _ in range(i):\n            st = time.perf_counter()\n            ret = func(a,b)\n            t = time.perf_counter() - st\n            total_time += t\n            if f is None or t < f:\n                f = t\n        return ret,f\n","metadata":{"id":"KGNUwFi4s86z","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tinygrad\n!pip install requests","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_BAMxf2WtTJ0","outputId":"4f327991-58b3-4af2-cf7e-6c8885ab303d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!/usr/bin/env python3 #for tinygrad repo, get rid of libs etc\n# can I beat https://github.com/jaymody/xpicoGPT.git?\n# beating https://github.com/WAUthethird/stupidGPT should be easy\nfrom typing import Union, Tuple\nfrom tqdm import trange\nimport numpy as np\nimport os\nimport pickle\nfrom tinygrad.nn.state import torch_load\nfrom tinygrad.helpers import fetch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport requests\nd = \"CUDA\"\nfolder = \"\"\n'''\ntry:\n  import pyopencl as cl\n  d = \"OpenCL\"\nexcept ImportError:\n   pass\n\ntry:\n  import Metal\n  import metal_kernels_large\n  d = \"Metal\"\n  folder = \"metal/\"\n  print(\"Using Metal\")\nexcept ImportError:\n  pass\n\ntry:\n  import pycuda.driver as cuda\n  import pycuda.autoinit\n  from pycuda.compiler import SourceModule\n  d = \"CUDA\"\nexcept ImportError:\n   pass\n'''\n\nd = \"CUDA\"\nif d == \"Metal\":\n    device = Metal.MTLCreateSystemDefaultDevice()\n    queue = device.newCommandQueue()\n    params = {\"queue\":queue,\"device\":device}\nif d == \"OpenCL\":\n    platform = cl.get_platforms()\n    my_gpu_devices = platform[0].get_devices(device_type=cl.device_type.GPU)\n    ctx = cl.Context(devices=my_gpu_devices)\n    mf = cl.mem_flags\n    params = {\"ctx\":ctx,\"mf\":mf,\"queue\":cl.CommandQueue(ctx)}\nif d == \"CUDA\":\n    params = None\n\n#https://raw.githubusercontent.com/roryclear/transformer/main/tokens.txt\n\n\n\nwith open(\"tokens2.txt\", \"wb\") as file:\n  file.write(requests.get(\"https://raw.githubusercontent.com/roryclear/transformer/main/tokens.txt\").content)\ntokens = open('tokens2.txt','r',encoding=\"utf-8\").readlines()\n\n#tokens = open('tokens.txt','r',encoding=\"utf-8\").readlines()\ntoken_dict = dict()\nmax_token_length = -1\nfor i in range(len(tokens)):\n  s = tokens[i].replace(\"\\n\",\"\").replace(\"/n\",\"\\n\")\n  token_dict[s] = i\n  if len(s) > max_token_length:\n    max_token_length = len(s)\ndef decode(index):\n  ret = \"\"\n  for i in index:\n    ret+=tokens[i].replace(\"\\n\",\"\").replace(\"/n\",\"\\n\") #hack with linebreak\n  return ret\n\ndef encode(x):\n  ret = []\n  token = None\n  i = -1\n  while len(x) > 0:\n    token = None\n    i = -1\n    while token == None:\n      i+=1\n      s = x[:min(max_token_length,len(x))-i]\n      if s in token_dict:\n        token = token_dict[s]\n    ret.append(token)\n    x = x[min(max_token_length,len(x))-i:]\n  return ret\n\n  def __call__():\n    return None\n\nclass Rand:\n  def __init__(self):\n    self.seed = 420\n\n  def rand(self):\n    self.seed += 1\n    rng = np.random.default_rng(self.seed)\n    rng_np_buffer = rng.random(size=1, dtype=np.float32).astype(dtype=np.float32, copy=False)\n    return rng_np_buffer[0]\n\nclass Transformer:\n  def __init__(self):\n    return None\n\n  def to_buffer(self,n_heads,dim):\n    self.n_heads = n_heads\n    self.dim = dim\n\n    print(\"copying ln_1_weight\")\n    for i in range(len(self.ln_1_weight)):\n      self.ln_1_weight[i] = create_buffer(self.ln_1_weight[i],d,params)\n\n    print(\"copying ln_1_bias\")\n    for i in range(len(self.ln_1_weight)):\n      self.ln_1_bias[i] = create_buffer(self.ln_1_bias[i],d,params)\n\n    print(\"copying attn_c_attn_weight\")\n    for i in range(len(self.ln_1_weight)):\n      self.attn_c_attn_weight[i] = create_buffer(self.attn_c_attn_weight[i].transpose(1,0).flatten(),d,params)\n\n    print(\"copying attn_c_attn_bias\")\n    for i in range(len(self.ln_1_weight)):\n      self.attn_c_attn_bias[i] = create_buffer(self.attn_c_attn_bias[i],d,params)\n\n    print(\"copying attn_c_proj_bias\")\n    for i in range(len(self.ln_1_weight)):\n      self.attn_c_proj_bias[i] = create_buffer(self.attn_c_proj_bias[i],d,params)\n\n    print(\"copying ln_2_weight\")\n    for i in range(len(self.ln_1_weight)):\n      self.ln_2_weight[i] = create_buffer(self.ln_2_weight[i],d,params)\n\n    print(\"copying ln_2_bias\")\n    for i in range(len(self.ln_1_weight)):\n      self.ln_2_bias[i] = create_buffer(self.ln_2_bias[i],d,params)\n\n    print(\"copying mlp_c_fc_bias\")\n    for i in range(len(self.ln_1_weight)):\n      self.mlp_c_fc_bias[i] = create_buffer(self.mlp_c_fc_bias[i],d,params)\n\n    print(\"copying mlp_c_proj_bias\")\n    for i in range(len(self.ln_1_weight)):\n      self.mlp_c_proj_bias[i] = create_buffer(self.mlp_c_proj_bias[i],d,params)\n\n    print(\"copying ln_f_weight\")\n    self.ln_f_weight = create_buffer(self.ln_f_weight,d,params)\n\n    print(\"copying ln_f_bias\")\n    self.ln_f_bias = create_buffer(self.ln_f_bias,d,params)\n\n    print(\"copying mlp_c_fc_weight\")\n    for i in range(len(self.ln_1_weight)):\n      self.mlp_c_fc_weight[i] = create_buffer(self.mlp_c_fc_weight[i].transpose(1,0).flatten(),d,params)\n\n    print(\"copying lm_head_weight_unf\")\n    self.lm_head_weight_unf = create_buffer(self.lm_head_weight.transpose(),d,params)\n\n    print(\"copying lm_head_weight\")\n    self.lm_head_weight = create_buffer(self.lm_head_weight.flatten(),d,params)\n\n    print(\"copying self_wte_weight\")\n    self.wte_weight = create_buffer(self.wte_weight.astype(np.float32),d,params)\n\n    print(\"copying self_wpe_weight\")\n    self.wpe_weight = create_buffer(self.wpe_weight,d,params)\n\n    print(\"creating attn_cache_kv\")\n    self.attn_cache_kv = []\n    for i in range(len(self.ln_1_weight)):\n      self.attn_cache_kv.append(create_buffer_empty(2*MAX_CONTEXT*n_heads*64*4,d,params))\n\n    if d == \"OpenCL\" or d==\"CUDA\":\n      print(\"copying attn_c_proj_weight\") #TODO\n      self.attn_c_proj_weight2 = []\n      for i in range(len(self.ln_1_weight)):\n        self.attn_c_proj_weight2.append(create_buffer(np.asfortranarray(self.attn_c_proj_weight[i]),d,params))\n        self.attn_c_proj_weight[i] = create_buffer(self.attn_c_proj_weight[i].flatten(),d,params)\n\n      print(\"copying mlp_c_proj_weight_unf\") #TODO\n      self.mlp_c_proj_weight_unf = []\n      for i in range(len(self.ln_1_weight)):\n        self.mlp_c_proj_weight_unf.append(create_buffer(np.asfortranarray(self.mlp_c_proj_weight[i]),d,params))\n        self.mlp_c_proj_weight[i] = create_buffer(self.mlp_c_proj_weight[i].flatten(),d,params)\n      return\n\n    if d == \"Metal\":\n      print(\"copying attn_c_proj_weight\") #TODO\n      self.attn_c_proj_weight2 = []\n      for i in range(len(self.ln_1_weight)):\n        self.attn_c_proj_weight2.append(create_buffer(np.asfortranarray(self.attn_c_proj_weight[i].transpose()),d,params))\n        self.attn_c_proj_weight[i] = create_buffer(self.attn_c_proj_weight[i],d,params)\n\n      print(\"copying mlp_c_proj_weight_unf\") #TODO\n      self.mlp_c_proj_weight_unf = []\n      for i in range(len(self.ln_1_weight)):\n        self.mlp_c_proj_weight_unf.append(create_buffer(self.mlp_c_proj_weight[i].transpose(),d,params))\n        self.mlp_c_proj_weight[i] = create_buffer(self.mlp_c_proj_weight[i].flatten(),d,params)\n\n  def forward(self, tokens, start_pos, temperature:float=0.8,n_tokens=444):\n    if start_pos > 0:\n      h = metalk.add(self.wte_weight,self.wpe_weight,start_pos,tokens[0])\n      attn_dim = self.dim\n      for i in range(0,len(self.ln_1_weight)):\n        h = metalk.kernel_0(h,self.ln_1_weight[i],\\\n        self.ln_1_bias[i],self.attn_c_attn_weight[i],\\\n        self.attn_c_attn_bias[i],\\\n        self.attn_cache_kv[i],\\\n        self.attn_c_proj_weight[i],self.attn_c_proj_bias[i],\\\n        self.ln_2_weight[i], self.ln_2_bias[i],\\\n        self.mlp_c_fc_weight[i],self.mlp_c_fc_bias[i],\\\n        self.mlp_c_proj_weight[i],self.mlp_c_proj_bias[i],start_pos,attn_dim,i)\n      unif_samples = rand.rand()\n      ret = metalk.kernel_1(h,self.ln_f_weight, self.ln_f_bias,self.lm_head_weight,temperature,unif_samples).astype(np.int32)[0]\n      return ret\n    else:\n      x = metalk.tok_emb(tokens,self.wte_weight,self.wpe_weight,n_tokens)\n      for i in range(len(self.ln_1_weight)-1):\n        x = metalk.kernel_2(x,self.ln_1_weight[i], self.ln_1_bias[i],self.attn_c_attn_weight[i],self.attn_c_attn_bias[i],self.attn_cache_kv[i],self.attn_c_proj_weight2[i],self.attn_c_proj_bias[i],self.ln_2_weight[i], self.ln_2_bias[i],\\\n        self.mlp_c_fc_weight[i],self.mlp_c_fc_bias[i],self.mlp_c_proj_weight_unf[i],self.mlp_c_proj_bias[i],n_tokens,MAX_CONTEXT,i)\n    unif_samples = rand.rand()\n    ret = metalk.kernel_3(x,self.ln_1_weight[-1], self.ln_1_bias[-1],self.attn_c_attn_weight[-1],self.attn_c_attn_bias[-1],self.attn_cache_kv[-1]\\\n    ,self.ln_f_weight, self.ln_f_bias,n_tokens,MAX_CONTEXT,self.lm_head_weight_unf,temperature,unif_samples).astype(np.int32)[0]\n    return ret\n\n  def __call__(self, tokens, start_pos, temperature:np.float32=0.0,n_tokens=1):\n    return self.forward(tokens, start_pos, temperature,n_tokens)\n\ndef delete_buffers(m): #TODO, do this with a loop\n    m.wpe_weight.delete()\n    m.ln_f_weight.delete()\n    m.ln_f_bias.delete()\n    m.wte_weight.delete()\n    m.lm_head_weight.delete()\n    m.lm_head_weight_unf.delete()\n    for x in range(len(m.ln_1_bias)): #TODO\n      m.mlp_c_proj_bias[x].delete()\n      m.mlp_c_proj_weight[x].delete()\n      m.mlp_c_proj_weight_unf[x].delete()\n      m.mlp_c_fc_bias[x].delete()\n      m.mlp_c_fc_weight[x].delete()\n      m.attn_c_proj_bias[x].delete()\n      m.attn_c_proj_weight[x].delete()\n      m.attn_c_proj_weight2[x].delete()\n      m.attn_c_attn_bias[x].delete()\n      m.attn_c_attn_weight[x].delete()\n      m.ln_1_bias[x].delete()\n      m.ln_1_weight[x].delete()\n      m.ln_2_bias[x].delete()\n      m.ln_2_weight[x].delete()\n      m.attn_cache_kv[x].delete()\n\nVOCAB_SIZE = 50257\nclass GPT2:\n  @staticmethod\n  def build():\n    model = Transformer(n_layers=12,n_heads=12,dim=768,norm_eps=1e-5,vocab_size=VOCAB_SIZE) #small\n    return GPT2(model)\n\n  def __init__(self, model):\n    self.model = model\n\n  def generate(self, prompt:str, max_length:int, temperature:float, timing:bool=False, batch_size:int=1,expected_tokens=None):\n    toks = encode(prompt)\n    start_pos = 0\n    n_tokens = len(toks)\n    for _ in trange(max_length, disable=(timing==True)):\n      if batch_size == 1 and len(toks[start_pos:]) == 1:\n        tokens = np.array([toks[start_pos]])\n      else:\n        tokens = np.array(toks)\n      tok = self.model(tokens, start_pos, temperature, n_tokens).tolist()\n      start_pos = len(toks)\n      if expected_tokens != None: #TODO REMOVE 13\n        np.testing.assert_equal(tok,expected_tokens[start_pos-n_tokens])\n      toks.append(tok)\n    return decode(toks)\n\n\ndef get_model(model_size):\n  gpt2_blank = GPT2(None)\n  gpt2_blank.model = Transformer()\n  n_layers = {\"gpt2\":12,\"gpt2-medium\":24,\"gpt2-large\":36,\"gpt2-xl\":48}\n  model = AutoModelForCausalLM.from_pretrained(model_size)\n  print(type(model))\n  print(\"converting wpe_weight\")\n  gpt2_blank.model.wpe_weight = model.transformer.wpe.weight.detach().cpu().numpy().astype(np.float32)\n\n  print(\"converting ln_f.weight\")\n  gpt2_blank.model.ln_f_weight = model.transformer.ln_f.weight.detach().cpu().numpy().astype(np.float32)\n\n  print(\"converting ln_f.bias\")\n  gpt2_blank.model.ln_f_bias = model.transformer.ln_f.bias.detach().cpu().numpy().astype(np.float32)\n\n  print(\"converting mlp_c_proj.bias\")\n  gpt2_blank.model.mlp_c_proj_bias = []\n  for x in range(n_layers[model_size]):\n      gpt2_blank.model.mlp_c_proj_bias.append(model.transformer.h[x].mlp.c_proj.bias.detach().cpu().numpy().astype(np.float32))\n\n  print(\"converting mlp_c_proj.weight\")\n  gpt2_blank.model.mlp_c_proj_weight = []\n  for x in range(n_layers[model_size]):\n      gpt2_blank.model.mlp_c_proj_weight.append(model.transformer.h[x].mlp.c_proj.weight.detach().cpu().numpy().astype(np.float32))\n\n  print(\"converting mlp_c_fc.bias\")\n  gpt2_blank.model.mlp_c_fc_bias = []\n  for x in range(n_layers[model_size]):\n      gpt2_blank.model.mlp_c_fc_bias.append(model.transformer.h[x].mlp.c_fc.bias.detach().cpu().numpy().astype(np.float32))\n\n  print(\"converting mlp_c_fc.weight\")\n  gpt2_blank.model.mlp_c_fc_weight = []\n  for x in range(n_layers[model_size]):\n      gpt2_blank.model.mlp_c_fc_weight.append(model.transformer.h[x].mlp.c_fc.weight.detach().cpu().numpy().astype(np.float32))\n\n  print(\"converting attn_c_proj.bias\")\n  gpt2_blank.model.attn_c_proj_bias = []\n  for x in range(n_layers[model_size]):\n      gpt2_blank.model.attn_c_proj_bias.append(model.transformer.h[x].attn.c_proj.bias.detach().cpu().numpy().astype(np.float32))\n\n  print(\"converting attn_c_proj.weight\")\n  gpt2_blank.model.attn_c_proj_weight = []\n  for x in range(n_layers[model_size]):\n      gpt2_blank.model.attn_c_proj_weight.append(model.transformer.h[x].attn.c_proj.weight.detach().cpu().numpy().astype(np.float32))\n\n  print(\"converting attn_c_attn.bias\")\n  gpt2_blank.model.attn_c_attn_bias = []\n  for x in range(n_layers[model_size]):\n      gpt2_blank.model.attn_c_attn_bias.append(model.transformer.h[x].attn.c_attn.bias.detach().cpu().numpy().astype(np.float32))\n\n  print(\"converting attn_c_attn.weight\")\n  gpt2_blank.model.attn_c_attn_weight = []\n  for x in range(n_layers[model_size]):\n      gpt2_blank.model.attn_c_attn_weight.append(model.transformer.h[x].attn.c_attn.weight.detach().cpu().numpy().astype(np.float32))\n\n  print(\"converting ln_1.bias\")\n  gpt2_blank.model.ln_1_bias = []\n  for x in range(n_layers[model_size]):\n      gpt2_blank.model.ln_1_bias.append(model.transformer.h[x].ln_1.bias.detach().cpu().numpy().astype(np.float32))\n\n  print(\"converting ln_1.weight\")\n  gpt2_blank.model.ln_1_weight = []\n  for x in range(n_layers[model_size]):\n      gpt2_blank.model.ln_1_weight.append(model.transformer.h[x].ln_1.weight.detach().cpu().numpy().astype(np.float32))\n\n  print(\"converting ln_2.bias\")\n  gpt2_blank.model.ln_2_bias = []\n  for x in range(n_layers[model_size]):\n      gpt2_blank.model.ln_2_bias.append(model.transformer.h[x].ln_2.bias.detach().cpu().numpy().astype(np.float32))\n\n  print(\"converting ln_2.weight\")\n  gpt2_blank.model.ln_2_weight = []\n  for x in range(n_layers[model_size]):\n      gpt2_blank.model.ln_2_weight.append(model.transformer.h[x].ln_2.weight.detach().cpu().numpy().astype(np.float32))\n\n  print(\"converting wte.weight\")\n  gpt2_blank.model.wte_weight = model.transformer.wte.weight.detach().cpu().numpy().astype(np.float32)\n\n  print(\"converting lm_head.weight\")\n  gpt2_blank.model.lm_head_weight = model.lm_head.weight.detach().cpu().numpy().astype(np.float32).transpose(1,0)\n\n  with open(folder+model_size+\".pickle\", 'wb') as outp:\n      pickle.dump(gpt2_blank, outp)\n\n# **** main code ****\n\nif __name__ == \"__main__\":\n  rand = Rand()\n\ndefault_prompt = \"What is the answer to life, the universe, and everything?\"\n#default_prompt = \"What happened in 1939?\"\n# should output:\n# .... The Jewish people rejected\n\n#(tg random) should output:\n#It was a very fateful day.\n#When the Nazis occupied Poland in 1939....\n\nnp.random.seed(28)\n\nexpected_tokens = [198, 198, 1532, 345, 547, 281, 48782,\\\n  893, 48187, 11, 393, 655, 257, 33013, 11, 534, 3280,\\\n  1244, 307, 257, 1643, 1180, 13, 1114, 530, 11, 345,\\\n  1244, 1011, 257, 2392, 1570, 286, 262, 6881, 13,\\\n  887, 329, 584, 661, 851, 1390, 5519, 11, 7912,\\\n  11, 290, 584, 287, 12, 14108, 12, 2550, 661, 851,\\\n  534, 3280, 1244, 307, 1290, 517, 588, 25, 5155, 1595,\\\n  470, 2152, 379, 477, 13, 198, 198, 25153, 345, 389, 257,\\\n  1862, 1048, 508, 655, 18303, 422, 3504, 1524, 290, 468,\\\n  1239, 1107, 19189, 257, 3451, 287, 48782, 23154, 13, 921, 821, 319, 281, 3624]\n\n#TODO, this is the current, but wrong output\nexpected_tokens_b = [198, 198, 1026, 373, 257, 845, 46873, 1110, 13, 198,\n198, 2215, 262, 19147, 12030, 12873, 287, 24414, 11, 262,\n6771, 547, 407, 3142, 284, 670, 287, 262, 17590, 11,\n645, 2300, 703, 881, 484, 2227, 284, 13, 383, 1917,\n2627, 1598, 618, 262, 5103, 1664, 286, 262, 309, 9116,\n4623, 268, 4618, 11, 543, 925, 281, 3113, 329, 262,\n11908, 12, 1273, 14414, 41460, 11, 3414, 617, 19008, 284,\n262, 24830, 4572, 13, 198, 198, 464, 4479, 286, 7570,\n21773, 2066, 82, 1908, 734, 11628, 284, 262, 31062, 13,\n1881, 373, 1912, 319, 257, 1080, 286, 2829, 12370, 4827]\n\nexpected_tokens_med = [198, 198, 1544, 468, 262, 2694, 290, 262, 481, 284, 3853, 475, 339, 2391,\\\n  2314, 2222, 2241, 284, 466, 340, 13, 679, 318, 7787, 284, 307, 3436, 290,\\\n  7787, 284, 2222, 1854, 656, 340, 13, 679, 318, 7787, 284, 307, 33046, 290,\\\n  7787, 284, 307, 8606, 13, 198, 198, 4864, 11, 339, 318, 407, 3436, 287,\\\n  465, 3252, 286, 5287, 13, 198, 198, 22210, 4952, 502, 326, 3252, 2125,\\\n  470, 262, 6808, 2728, 286, 262, 1917, 13, 198, 198, 2025, 37848, 284,\\\n  674, 10251, 481, 1282, 611, 356, 12553, 262, 4950, 2000, 1176, 356,\\\n  423, 13, 198, 198, 2215, 345]\n\nexpected_tokens_large = [198, 198, 1532, 345, 550, 257, 40663, 11, 345, 561,\n2192, 1382, 340, 656, 262, 6766, 13, 2293, 477, 11,\n345, 714, 655, 4829, 262, 1468, 2272, 18556, 656, 262,\n8137, 290, 1956, 340, 7382, 13, 198, 198, 1537, 326,\n40663, 561, 779, 262, 976, 3716, 1080, 284, 366, 33327,\n1, 262, 3404, 319, 262, 4417, 286, 262, 5440, 13,\n921, 460, 470, 2824, 3404, 416, 17997, 3404, 656, 262,\n1633, 960, 14108, 40663, 318, 517, 588, 257, 16285, 508,\n46561, 262, 3404, 510, 422, 262, 2323, 13, 5455, 11,\n345, 561, 7925, 1657, 12, 6551, 5696, 422, 262, 3668]\n\n#a = create_buffer_empty(1*4,d,params) #TODO can't run medium in isolation without doing this first?\nrand = Rand()\nMAX_CONTEXT = len(encode(default_prompt))+100\nmetalk = Kernels(dim=768,n_heads=12,max_context=MAX_CONTEXT,device=d)\nif os.path.exists(folder+\"gpt2.pickle\") == False:\n  get_model(\"gpt2\")\nfilehandler = open(folder+\"gpt2.pickle\", 'rb')\ngpt2 = pickle.load(filehandler)\ngpt2.model.to_buffer(12,768)\ntext = gpt2.generate(prompt=default_prompt, max_length=100, temperature=np.float32(0.8), timing=None, batch_size=1,expected_tokens=None)\nprint((f\"Response:\", \"green\"), text)\ndelete_buffers(gpt2.model)\n\nrand = Rand()\nMAX_CONTEXT = len(encode(\"What happened in 1939?\"))+100\nmetalk = Kernels(dim=768,n_heads=12,max_context=MAX_CONTEXT,device=d)\nfilehandler = open(folder+\"gpt2.pickle\", 'rb')\ngpt2 = pickle.load(filehandler)\ngpt2.model.to_buffer(12,768)\ntext = gpt2.generate(prompt=\"What happened in 1939?\", max_length=100, temperature=np.float32(0.8), timing=None, batch_size=1,expected_tokens=None)\nprint((f\"Response:\", \"green\"), text)\ndelete_buffers(gpt2.model)\n\nMAX_CONTEXT = len(encode(default_prompt))+100\nmetalk = Kernels(dim=1024,n_heads=16,max_context=MAX_CONTEXT,device=d)\nif os.path.exists(folder+\"gpt2-medium.pickle\") == False:\n  get_model(folder+\"gpt2-medium\")\nfilehandler = open(folder+\"gpt2-medium.pickle\", 'rb')\ngpt2 = pickle.load(filehandler)\n#gpt2.model.to_buffer2()\ngpt2.model.to_buffer(16,1024)\nrand = Rand()\ntext = gpt2.generate(prompt=default_prompt, max_length=100, temperature=np.float32(0.8), timing=None, batch_size=1,expected_tokens=None)\nprint((f\"Response:\", \"green\"), text)\ndelete_buffers(gpt2.model)\n\nMAX_CONTEXT = len(encode(default_prompt))+100\ndim = 1280\nn_heads = 20\nrand = Rand()\nmetalk = Kernels(dim=1280,n_heads=20,max_context=MAX_CONTEXT,device=d)\nif os.path.exists(folder+\"gpt2-large.pickle\") == False:\n  get_model(\"gpt2-large\")\nfilehandler = open(folder+\"gpt2-large.pickle\", 'rb')\ngpt2 = pickle.load(filehandler)\ngpt2.model.to_buffer(20,1280)\ntext = gpt2.generate(prompt=default_prompt, max_length=100, temperature=np.float32(0.8), timing=None, batch_size=1,expected_tokens=None)\nprint((f\"Response:\", \"green\"), text)\ndelete_buffers(gpt2.model)\n\nif d == \"Metal\":\n  MAX_CONTEXT = len(encode(default_prompt))+100\n  dim = 1280\n  n_heads = 20\n  metalk = metal_kernels_large.Metal_Kernels(dim=1280,n_heads=20,max_context=MAX_CONTEXT)\n  if os.path.exists(folder+\"gpt2-large.pickle\") == False:\n    get_model(\"gpt2-large\")\n  filehandler = open(folder+\"gpt2-large.pickle\", 'rb')\n  gpt2 = pickle.load(filehandler)\n  gpt2.model.to_buffer(20,1280)\n  rand = Rand()\n  text = gpt2.generate(prompt=default_prompt, max_length=100, temperature=np.float32(0.8), timing=None, batch_size=1,expected_tokens=None)\n  print((f\"Response:\", \"green\"), text)\n  delete_buffers(gpt2.model)\n\n''' TODO\nMAX_CONTEXT = len(encode(default_prompt))+100\ndim = 1600\nn_heads = 25\nmetalk = metal_Metal_Kernels(dim=1600,n_heads=25,max_context=MAX_CONTEXT)\nif os.path.exists(\"gpt2-xl.pickle\") == False:\n  get_model(\"gpt2-xl\")\nfilehandler = open(\"gpt2-xl.pickle\", 'rb')\ngpt2 = pickle.load(filehandler)\ngpt2.model.to_buffer(25,1600)\nrand = Rand()\ntext = gpt2.generate(prompt=default_prompt, max_length=100, temperature=np.float32(0.8), timing=None, batch_size=1,expected_tokens=None)\nprint((f\"Response:\", \"green\"), text)\ndelete_buffers(gpt2.model)\n'''","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b4a8ad4c627f4aad9da2cba0f40accad","40d843903ace4b52873231bd579ed8d4","56f7eff3723349768f2892bdc8fefdc0","754f0938ecba471286e03998bcaa8301","dbc566d3c9fa42d0b46c9228ae0727cf","31eba0d68e28474cbf339b3ecbb6361f","42281e6a6f8e4b52b48999a169bf3865","f41e5cb8188d4322b867a4f875537e75","f0e0b901d9664bc582f22b754f9867df","81c1f175d292466f8cc33af88f67e64b","8831aa33d12b4358b2d1a6cba37a9b68","fe628553dd864973bf20cf3ce65079b4","97ad8596ca4e451fab1e27b77042bff2","d546eab1455c4d13a76c6ac7866e97d8","dc12da618aac480e8af10e6c1d952b23","eb696ba947a648bebf3e49cd46035e1a","1e34eae510bd45c6851c807133050fbc","cd19335c162e43e3be36837774788809","9959f4ae610b4e9e8ecc8e46a8c5a695","1bdcab6db320413bacad142669c3dae9","4e680250581c49a1a834d85a90fada2d","b7b53a4c6d2848d4b1475e95e22f2b6b","8e2d449b69bb48cc9a3bdd4dd2d15ced","efc4df58d52842e0af3137a382ab6e1d","f0b700a699fd467a80f181751655bc03","f80d1deae6f7410b9310632f28b9028e","59fe6e2f0bbb4f608de68910d5202849","d845a17383954464b27462173cc8fa06","7d0c7a330ccc4422b7cb83d6b6c361e1","3e19008db44d4fbca9128140f7b1a028","abd7d7f1ff3e4972ba7bf8cbf0294fd9","34a79bed6abd48f794b2c698fb5aabd7","b718964f1414467ca3f770546f7b412a"]},"id":"UOg7Mb06tKK7","outputId":"54102541-7f63-4dce-ebff-4114cf95de23","trusted":true},"execution_count":null,"outputs":[]}]}