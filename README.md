# Transformer
Use git lfs to clone with weights
## Fastest OpenCL GPT-2 (124M) inference*
- [x] Fastest Inference*

- [X] Support for all GPT-2 Models

- [ ] Remove all Numpy Usage

- [ ] Support for any transformer

- [ ] Support other compute languages

- [ ] Refactor to library

  

*fastest GPT-2 124M inference I can find, running on my laptop (2020 XPS13). Almost twice as fast as huggingface's Transformers. 128 tokens: 15 vs 28 seconds.
